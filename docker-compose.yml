services:
  # LLM Server (CPU-based with llama.cpp)
  llama:
    image: ${LLAMA_REPO:-ghcr.io/ggerganov/llama.cpp:server}@${LLAMA_DIGEST:-sha256:42d562e394e22fc1b05d6f0ee9179b276b80a115217c8d668dc8c2fc5b1302ac}
    container_name: cintara-llm
    restart: unless-stopped
    command: [
      "--model", "/models/${MODEL_FILE:-tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf}",
      "--ctx-size", "${CTX_SIZE:-256}",
      "--threads", "${LLM_THREADS:-1}",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--n-predict", "32",
      "--batch-size", "64",
      "--timeout", "30"
    ]
    volumes:
      - ./models:/models:ro
    environment:
      - LLAMA_ARG_HOST=0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/health > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    ports:
      - "8000:8000"  # LLM API port
    deploy:
      resources:
        limits:
          cpus: '1.0'  # Further reduced for stability
          memory: 1.5G # Conservative memory limit
        reservations:
          cpus: '0.25' # Minimal reservation
          memory: 512M # Minimal memory

  # AI Bridge - Connects to host Cintara node
  bridge:
    build: ./bridge
    container_name: cintara-ai-bridge
    restart: unless-stopped
    network_mode: "host"  # Use host networking for better connectivity to host services
    ports:
      - "8080:8080"  # Explicitly bind bridge API port
    volumes:
      - /data/.tmp-cintarad/data:/data/.tmp-cintarad/data:ro  # Mount LevelDB data directory read-only
      - /home/ubuntu/.cintarad/data:/home/ubuntu/.cintarad/data:ro  # Alternative path if using home directory
    user: "0:0"  # Run as root to access root-owned database files
    environment:
      - LLAMA_SERVER_URL=http://localhost:8000
      - CINTARA_NODE_URL=http://localhost:26657  # Connect to host node via localhost
      - LOG_PATH=/app/logs
      - AI_FEATURES_ENABLED=true
    depends_on:
      llama:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health > /dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 2
      start_period: 30s

  # Web UI - Simple HTML interface (reliable, no build issues)
  web-ui:
    build:
      context: ./web-ui  
      dockerfile: Dockerfile.simple
    container_name: cintara-web-ui
    restart: unless-stopped
    network_mode: "host"
    ports:
      - "3000:3000"  # Explicitly bind web UI port
    depends_on:
      bridge:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # React Web UI - Advanced interface (only if build issues are resolved)
  web-ui-react:
    build: 
      context: ./web-ui
      dockerfile: Dockerfile
    container_name: cintara-web-ui-react
    restart: unless-stopped
    network_mode: "host"
    environment:
      - REACT_APP_API_URL=http://localhost:8080
      - NODE_ENV=production
    depends_on:
      bridge:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - react